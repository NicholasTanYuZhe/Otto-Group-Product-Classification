#Assignment 3 - Classification
#Otto Group Product Classification

##Dataset Description##

<p>The dataset we choose to use for this assignment is Otto Group Product dataset which is obtained from <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge">Kaggle</a>.There are total of 95 columns and 61878 rows.</p>
Data fields:
<ul>
  <li>**id** - an annoymous id that is unique to a product</li>
  <li>**feat_1, feat_2, ..., feat_93** - the various features of a product</li>
  <li>**target** - the class of a product</li>
</ul>

---

##Libraries needed##
```{r message=FALSE}
library(caret)
library(tree)
library(randomForest)
library(e1071)
library(nnet)
library(corrplot)

#import function from Github to plot nnet model
require(RCurl)

root.url<-'https://gist.githubusercontent.com/fawda123'
raw.fun<-paste(
  root.url,
  '5086859/raw/cc1544804d5027d82b70e74b83b3941cd2184354/nnet_plot_fun.r',
  sep='/'
)
script<-getURL(raw.fun, ssl.verifypeer = FALSE)
eval(parse(text = script))
rm('script','raw.fun')
```

---

##Characteristics and summary of dataset
```{r message=FALSE}
set.seed(1022)
train_df <- read.csv("train.csv")
```
```{r}
str(train_df)
```
```{r}
summary(train_df)
```
```{r}
head(train_df)
```
```{r}
#Find out the correlation between the columns
corr <- cor(train_df[,1:94])
corrplot(corr, method="square", type="lower")
```
```{r}
#To find out the amount of each class have
summary(train_df$target)
```
---

##Preprocessing of dataset
<p>We check is there any missing data in the dataset</p>
```{r}
#Check for missing value
apply(train_df,2,function(x) sum(is.na(x)))
```
<p>There is no missing value in the dataset.</p>
<p>We also remove the id column</p>
```{r}
#Remove id because it is the same with the row number
train_df <- train_df[,-1]
```

---

<h3>Choice of performance measures</h3>
<p>Our choice of performance measures will be accuracy, sensitivity and specificity. We will going to use three of these to compare the results of models. We will mostly focus on accuracy for the evaluation part.</p>

---
## Preparation of data and performance of models {.tabset .tabset-fade .tabset-pills}

### Tree

#### Preparing data for Tree
```{r}
#Preparing data for tree
train_size <- floor(0.75 * nrow(train_df))
train_ind <- sample(seq_len(nrow(train_df)), size = train_size)
train_tree <- train_df[train_ind, ]
test_tree <- train_df[-train_ind, ]
```
##### {.tabset .tabset-fade .tabset-pills}
###### Performance of Decision Tree (Tree)
```{r}
#Tree
tree_model <- tree(train_tree$target ~ ., data=train_tree)
plot(tree_model)
title(main="tree")
text(tree_model)
```
<p>We can take a look on the performance of the tree</p>
```{r}
#Test the model with predict
tree_pred <- predict(tree_model, test_tree, type="class")

#Create confusion matrix 
confusionMatrix(tree_pred, test_tree$target)
```
<p>Implement cross validation</p>
```{r}
tree_cv <- cv.tree(tree_model, FUN=prune.misclass)
names(tree_cv)
plot(tree_cv$size, tree_cv$dev, type="b")
plot(tree_cv$k, tree_cv$dev, type="b")
plot.tree.sequence(tree_cv)
```
<p>Now we try to prune the tree to see whether or not it have any impact on the performance</p>
```{r}
#Prune the tree
pruned_model <- prune.misclass(tree_model, best=9)
plot(pruned_model)
text(pruned_model)
```
<p>We can take a look on the performance of the tree after pruning</p>
```{r}
tree_pred <- predict(pruned_model, test_tree, type="class")
confusionMatrix(tree_pred, test_tree$target)

summary(pruned_model)
```
<p>Basically even after we prune the tree, the performance did not increase and stay on 0.576 of accuracy which is very low. So we explore another type of tree using randomForest</p>
###### Performance of Decision Tree (randomForest)
```{r}
#Calculating ideal value for mtry parameter
#mtry <- tuneRF(train_tree[,1:93], train_tree[,94], mtryStart=1,
#              ntreeTry=50, stepFactor=2, improve=0.05,
#              trace=TRUE, plot=TRUE, doBest=FALSE)

#Best value for mtry is 8

tree_model2 <- randomForest(train_tree$target ~ ., data=train_tree,
                            importance=TRUE,ntree=50,mtry=8)
#Create dotchart of variable/feature importance that is measured by random forest
varImpPlot(tree_model2)
```
![](Feature_Importance(randomForest).png)
```{r}
tree_pred2 <- predict(tree_model2, test_tree, type="response")
confusionMatrix(tree_pred2, test_tree$target)
```

### Naive Bayes

#### Preparing data for Naive Bayes
```{r}
#Preparing data for naive bayes
#Convert the whole data frame to factor
train_df_factor <- as.data.frame(lapply(train_df, function(x) as.factor(x)))
train_ind_factor <- sample(seq_len(nrow(train_df_factor)), size = train_size)
train_NB <- train_df[train_ind_factor, ]
test_NB <- train_df[-train_ind_factor, ]
```

#### Performance of Naive Bayes
```{r}
#Naive Bayes
NB_model <- naiveBayes(train_NB$target ~ ., train_NB, laplace = 1)
NB_model

NB_pred <- predict(NB_model, test_NB[,-(ncol(test_NB))])
confusionMatrix(NB_pred, test_NB$target)
```


### Artificial Neural Network

#### Preparing data for Artificial Neural Network
```{r}
#Preparing data for ANN
train_ANN <- train_df[train_ind, ]
test_ANN <- train_df[-train_ind, ]
```

#### Performance of Artificial Neural Network
```{r}
NN_model <- nnet(train_ANN$target ~ ., train_ANN[,-1], size = 3, rang = 0.1, decay = 5e-4, maxit = 500)
plot.nnet(NN_model)
# Compute Predictions off Test Set
NN_pred <- predict(NN_model, test_ANN[,1:93],type="class")
confusionMatrix(test_ANN$target,NN_pred)
```

---

## Comparision between 3 classifier
<table>
|             | Decision Tree (Tree) | Decision Tree (randomForest) | Naive Bayes | Artificial Neural Network |
|-------------|----------------------|------------------------------|-------------|---------------------------|
| Accuracy    | 0.576                | 0.804                        | 0.584       | 0.724                     |
| Sensitivity | 0.404                | 0.714                        | 0.551       | 0.634                     |
| Specificity | 0.940                | 0.973                        | 0.947       | 0.967                     |
</table>

## Suggestion as to why the classifiers behave differently
